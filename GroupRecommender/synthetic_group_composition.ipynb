{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Recommender System\n",
    "\n",
    "This file contains an aggregation algorithm for groups of users, aswell as its evaluation, explanations and evaluations of explanations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lenskit import topn\n",
    "from lenskit import crossfold as xf\n",
    "from lenskit import batch, topn, util\n",
    "from lenskit.algorithms import Recommender, user_knn as uknn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning, module='lenskit.metrics.topn')\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='lenskit.metrics.topn')\n",
    "from surprise.model_selection import cross_validate\n",
    "from lenskit.algorithms import Recommender, bias, basic, item_knn\n",
    "from lenskit.algorithms.user_knn import UserUser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the dataset and preprocess the data. More specifications about this can be found in the \"data_preprocessing.ipynb\" file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    preprocessed_dataset_folder = \"../Data/PreprocessedDataset\"\n",
    "    ratings_df = pd.read_csv(preprocessed_dataset_folder+\"/ratings.csv\")\n",
    "    movies_df = pd.read_csv(preprocessed_dataset_folder+\"/movies.csv\")\n",
    "    user_plots_ratings_df = pd.read_csv(preprocessed_dataset_folder+\"/user_plots.csv\") #first run notebook algorithm_experiments to get this data\n",
    "    return ratings_df, movies_df, user_plots_ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic group generation\n",
    "\n",
    "Opposed to using user-user similarity metric (e.g PCC) to create groups, we have decided to try something new (K-Means clustering algorithm).\n",
    "\n",
    "1. The initial goal is to segment the data into 4 distinct groups based on user features in the user matrix, this is the ratings for different items. \n",
    "- In order to achieve a proper split, we run k_means multiple times (100 in our experiments), and select the result with the lowest inertia (sum of squared distances from each point to its assigned center).\n",
    "\n",
    "\n",
    "\n",
    "Before running the k-means algorithm, we must first fill in the user_matrix with predicted ratings in case of a non-rated movie. \n",
    "\n",
    "We do this to avoid a huge amount of sparsity when performing segmentation. We use collaborative filtering (kNN, UserUser) from the individual recommender in order to perform these predicitons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_filtering(data):\n",
    "    algo = UserUser(15,min_nbrs=3) \n",
    "    algo = Recommender.adapt(algo)\n",
    "    algo.fit(data)\n",
    "\n",
    "    user_ids = data['user'].unique()  \n",
    "    item_ids = data['item'].unique() \n",
    "\n",
    "    all_preds = []\n",
    "\n",
    "    for user in user_ids:\n",
    "        preds = algo.predict_for_user(user, item_ids)\n",
    "        for item, pred in zip(item_ids, preds):\n",
    "            if not np.isnan(pred): \n",
    "                all_preds.append([user, item, pred])\n",
    "\n",
    "    pred_df = pd.DataFrame(all_preds, columns=['user', 'item', 'rating'])\n",
    "    filled_user_matrix = pred_df.pivot(index='user', columns='item', values='rating')\n",
    "    filled_user_matrix.fillna(0, inplace=True)\n",
    "    return filled_user_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, movies_df, user_plots_ratings_df = load_dataset()\n",
    "user_matrix_filled = collaborative_filtering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer     # METHOD USED FOR GROUND TRUTH\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Select movies rated by the user\n",
    "def getRecommendations(user_plots_ratings_df, user_id):\n",
    "    user_rated_movies = user_plots_ratings_df[user_plots_ratings_df['user_id'] == user_id]\n",
    "    X = user_rated_movies['plot + title + genres']\n",
    "    X_labels = user_rated_movies['item']\n",
    "    y = user_rated_movies['rating']\n",
    "    X_not_rated_labels = user_plots_ratings_df[user_plots_ratings_df['user_id'] != user_id]['item'].unique()\n",
    "    X_not_rated = user_plots_ratings_df[user_plots_ratings_df['user_id'] != user_id]['plot + title + genres'].unique()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(X)\n",
    "    X_not_rated_tfidf = vectorizer.transform(X_not_rated)\n",
    "    neigh = KNeighborsRegressor(n_neighbors=3, metric='cosine')\n",
    "    neigh.fit(X_tfidf, y)\n",
    "    y_pred = neigh.predict(X_not_rated_tfidf)\n",
    "    not_rated_movies_predictions = pd.DataFrame({\n",
    "        'item': user_plots_ratings_df[user_plots_ratings_df['user_id'] != user_id]['item'].unique(),\n",
    "        'predicted_rating': y_pred\n",
    "    })\n",
    "    recommended_movies = pd.merge(not_rated_movies_predictions, movies_df, on='item')\n",
    "    recommended_movies = recommended_movies.sort_values(by='predicted_rating', ascending=False).head(5)\n",
    "    cosine_sim_matrix = cosine_similarity(X_not_rated_tfidf, X_tfidf)\n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=X_not_rated_labels, columns=X_labels)\n",
    "    return cosine_sim_df, recommended_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means for similarity based groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(n_of_clusters, n_runs=100):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(user_matrix_filled)\n",
    "\n",
    "    best_centers = None\n",
    "    best_labels = None\n",
    "    min_inertia = float('inf')\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        kmeans = KMeans(n_clusters=n_of_clusters, random_state=i).fit(scaled_data)\n",
    "        if kmeans.inertia_ < min_inertia:\n",
    "            min_inertia = kmeans.inertia_\n",
    "            best_centers = kmeans.cluster_centers_\n",
    "            best_labels = kmeans.labels_\n",
    "\n",
    "    user_matrix_filled['cluster'] = best_labels\n",
    "    grouped = user_matrix_filled.groupby('cluster')\n",
    "\n",
    "    return best_centers, grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-medoids for similarity based groups\n",
    "\n",
    "This method was implemented at the beginning of the project with the aim to handle sparse data. Since there are thousands of users and movies, there is a significant large amount of unrated movies per user. Our goal of using a clustering algorithm remained, therefore we attempted to use the k-Medoids variation. Another reason to implement k-medoids was to use PCC as a distance metric opposed to k-means that uses euclidean distance. \n",
    "\n",
    "K-Medoids: K-Medoids clustering algorithm partitions data into k groups, selecting actual data points as centers (medoids). It's robust to outliers, handles various distance metrics, and is suitable for sparse or dissimilar data, albeit computationally intensive compared to K-Means.\n",
    "\n",
    "This method now remains unused as we have decided to fill the non-rated items with predicted ratings using the collaborative filtering (kNN). Data was no longer imensely sparse and k-means was providing a better segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "def pcc_distance(X, Y=None):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    n = X.shape[0]\n",
    "    m = Y.shape[0]\n",
    "    result = np.empty((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            x = np.atleast_1d(X[i])\n",
    "            y = np.atleast_1d(Y[j])\n",
    "            result[i, j] = 1 - pearsonr(x, y)[0]\n",
    "    return result\n",
    "\n",
    "def k_medoids(n_of_clusters, n_runs=100):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(user_matrix_filled)\n",
    "\n",
    "    best_centers = None\n",
    "    best_labels = None\n",
    "    min_inertia = float('inf')\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        kmedoids = KMedoids(n_clusters=n_of_clusters ,random_state=i).fit(scaled_data)\n",
    "        if kmedoids.inertia_ < min_inertia:\n",
    "            min_inertia = kmedoids.inertia_\n",
    "            best_centers = kmedoids.cluster_centers_\n",
    "            best_labels = kmedoids.labels_\n",
    "\n",
    "    user_matrix_filled['cluster'] = best_labels\n",
    "    grouped = user_matrix_filled.groupby('cluster')\n",
    "\n",
    "    return best_centers, grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the 4 clusters. The final choice is to use k_means, and obtain the result with the lowest inertia after 100 runs. \n",
    "\n",
    "The decisions on 4 clusters and k_means(opposed to k_medoids) are backed up with 2 experiments performed later in this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_centers, grouped = k_means(4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Strategies\n",
    "\n",
    "Group recommender systems aggregate preferences of group members to generate recommendations that are likely to satisfy the group as a whole.\n",
    "We initially implemented 5 strategies:\n",
    "\n",
    "- Additive Ultiritarian \n",
    "- Approval Voting\n",
    "- Least Misery \n",
    "- Majority \n",
    "- Most Pleasure\n",
    "\n",
    "Now before proceeding, let us come back to a decision taken previously. Why do we use collaborative filtering to fill in empty values? And what difference does it make?\n",
    "\n",
    "- The main reason is to improve group recommendations. With more ratings available for each user, aggregation strategies have more data to work with when forming group recommendations. Each user in the group is also better represented, leading to recommendations that potentially satisfy all group members. Filling in missing values provides a more complete user-item interaction matrix. \n",
    "\n",
    "- Handling Sparsity: User-item matrices are typically sparse, meaning most users have rated only a small fraction of items. Collaborative filtering helps predict these missing ratings, making the matrix denser. This was a choice taken after our alternative to handle sparse data did not perform up to expectations and as mentioned before, we want to stick to our clustering approach for group composition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Average strategy - Simply calculate the mean rating for each movie by all users in the group and get the top 10 rated ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Additive Utilitarian (ADD) is a consensus-based strategy that considers the preferences of all group members and recommends the item with the highest sum of all group members’ ratings (Senot et al. 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_utilitarian_strategy(group, n_movies):\n",
    "\n",
    "    if 'cluster' in group.columns:\n",
    "        group = group.drop(columns=['cluster'])\n",
    "\n",
    "    total_ratings = group.sum(axis=0)\n",
    "    sorted_movies = total_ratings.sort_values(ascending=False)\n",
    "    return (sorted_movies.head(n_movies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "• Approval Voting (APP) is a majority-based strategy, focusing on the most popular items among group members, recommending the item with the highest number of ratings above a predefined threshold (Senot et al. 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approval_voting_strategy(group, n_movies):         \n",
    "    if 'cluster' in group.columns:\n",
    "        group = group.drop(columns=['cluster'])\n",
    "\n",
    "    approval_threshold = 4 \n",
    "    approved_items_count = (group > approval_threshold).sum(axis=0)\n",
    "    sorted_movies = approved_items_count.sort_values(ascending=False)\n",
    "    \n",
    "    return (sorted_movies.head(n_movies))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Least Misery (LMS) is a borderline strategy, considering only a subset of group members’ preferences and recommends the item which has the highest of all lowest ratings (Senot et al. 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_misery_strategy(group, n_movies):\n",
    "\n",
    "    if 'cluster' in group.columns:\n",
    "        group = group.drop(columns=['cluster'])\n",
    "\n",
    "    min_ratings = group.min(axis=0)\n",
    "    sorted_movies = min_ratings.sort_values(ascending=False)\n",
    "    return sorted_movies.head(n_movies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Majority(MAJ) is a border line strategy that recommends the item with the highest number of all ratings representing the majority of item-specific ratings (Senot et al.2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_strategy(group, n_movies, threshold=4): \n",
    "    likes = group[group >= threshold]\n",
    "    like_counts = likes.count()\n",
    "    majority_threshold = len(group) / 2\n",
    "    majority_likes = like_counts[like_counts > majority_threshold]\n",
    "    return majority_likes.nlargest(n_movies)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Most Pleasure (MPL) is a borderline strategy that recommends the item with the\n",
    "highest individual group member rating (Senot et al. 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_pleasure_strategy(group, n_movies):\n",
    "\n",
    "    if 'cluster' in group.columns:\n",
    "        group = group.drop(columns=['cluster'])\n",
    "\n",
    "    max_ratings = group.max(axis=0)\n",
    "    sorted_movies = max_ratings.sort_values(ascending=False)\n",
    "    return sorted_movies.head(n_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Compositions\n",
    "\n",
    "When it comes to group compositions, we have tried 4 different techniques: Divergent, Uniform, Coalitional, and Minority. \n",
    "A quick overview of how each works is displayed bellow next to the corresponding techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divergent group configuration: Characterized by a high internal diversity between group members’ preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergent_groups(clusters, num_of_groups):\n",
    "    all_groups = {}\n",
    "\n",
    "    for i in range(num_of_groups):\n",
    "        sampled_users = []\n",
    "\n",
    "        for cluster_id, group in clusters: \n",
    "            if not group.empty:\n",
    "                sampled_user = group.sample(n=1)\n",
    "                sampled_users.append(sampled_user)\n",
    "\n",
    "        group_of_sampled_users = pd.concat(sampled_users)\n",
    "        all_groups[f'group_{i+1}'] = group_of_sampled_users\n",
    "\n",
    "    return all_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform group configuration: characterized by a low internal diversity between group members’ preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_groups(clusters, num_of_groups, group_size=4):\n",
    "    all_groups = {}\n",
    "    group_count = 0\n",
    "\n",
    "    while group_count < num_of_groups:\n",
    "            for cluster_id, group in clusters:\n",
    "                if len(group) >= group_size:\n",
    "                    sampled_users = group.sample(n=group_size)\n",
    "                    group_name = f\"Group_{group_count+1}_from_Cluster_{cluster_id}\"\n",
    "                    all_groups[group_name] = sampled_users\n",
    "                    group_count += 1\n",
    "\n",
    "                    if group_count >= num_of_groups:\n",
    "                        break\n",
    "        \n",
    "    return all_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coalitional group configuration: Characterized by two disjoint subgroups having low inter-group diversity and high intra-group diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalitional_groups(clusters, num_of_groups, group_size=4):\n",
    "    all_groups = {}\n",
    "    group_count = 0\n",
    "\n",
    "    cluster_list = [group for _, group in clusters if len(group) >= 2]\n",
    "\n",
    "    while group_count < num_of_groups:\n",
    "        for cluster_combination in itertools.permutations(cluster_list, 2):\n",
    "            cluster_a, cluster_b = cluster_combination\n",
    "\n",
    "            if len(cluster_a) >= 2 and len(cluster_b) >= 2:\n",
    "                sampled_users_a = cluster_a.sample(n=2)\n",
    "                sampled_users_b = cluster_b.sample(n=2)\n",
    "                group_of_sampled_users = pd.concat([sampled_users_a, sampled_users_b])\n",
    "                all_groups[f'group_{group_count+1}'] = group_of_sampled_users\n",
    "                group_count += 1\n",
    "\n",
    "                if group_count >= num_of_groups:\n",
    "                    break\n",
    "\n",
    "    return all_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minority group configuration: characterized by a subgroup with N-1 users with low internal diversity, where all the N-1 users have a high diversity with the remaining user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minority_groups(clusters, num_of_groups, group_size=4):\n",
    "    all_groups = {}\n",
    "    group_count = 0\n",
    "\n",
    "    viable_clusters = [group for _, group in clusters if len(group) >= 1]\n",
    "\n",
    "    while group_count < num_of_groups:\n",
    "        for cluster_combination in itertools.permutations(viable_clusters, 2):\n",
    "            cluster_a, cluster_b = cluster_combination\n",
    "            if len(cluster_a) >= group_size - 1 and len(cluster_b) >= 1:\n",
    "                sampled_users_a = cluster_a.sample(n=group_size - 1)\n",
    "                sampled_user_b = cluster_b.sample(n=1)\n",
    "                group_of_sampled_users = pd.concat([sampled_users_a, sampled_user_b])\n",
    "                all_groups[f'group_{group_count+1}'] = group_of_sampled_users\n",
    "                group_count += 1\n",
    "\n",
    "                if group_count >= num_of_groups:\n",
    "                    break\n",
    "\n",
    "    return all_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More decisions\n",
    "\n",
    "- The K-means algorithm was chosen over K-medoids for the purpose of creating four distinct user clusters based on movie ratings. K-means minimizes the within-cluster variance, while K-medoids minimizes the sum of dissimilarities between objects labeled to be in a cluster and the medoid of that cluster.Given that movie ratings are continuous, K-means, which assigns cluster centers as the mean value of the points within each cluster, is more naturally suited to the data set.\n",
    "\n",
    "- We chose Euclidean distance over Pearson Correlation Coefficient (PCC) to continue with the K-means clustering approach. Although PCC measures linear correlations, Euclidean distance effectively minimizes variance within clusters, ensuring precise data partitioning and cohesive cluster formation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snipet of code below is simply measuring the average similarity between every pair of clusters. Used to test if k-means segmentation was working properly for different cluster sizes. 4 clusters was the most appropriate find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 15.931156568434735\n",
      "0 1 35.33325849714013\n",
      "0 2 47.97501542192275\n",
      "0 3 23.59319209361195\n",
      "1 0 35.33325849714016\n",
      "1 1 10.953594511600656\n",
      "1 2 16.973071385776016\n",
      "1 3 16.533687895741142\n",
      "2 0 47.975015421922706\n",
      "2 1 16.973071385776223\n",
      "2 2 12.602432842416743\n",
      "2 3 27.85168780978032\n",
      "3 0 23.593192093611915\n",
      "3 1 16.5336878957411\n",
      "3 2 27.851687809780728\n",
      "3 3 11.342199891513843\n"
     ]
    }
   ],
   "source": [
    "for cluster,group in grouped:\n",
    "    group = group.drop(columns=['cluster'])\n",
    "    for cluster2,group2 in grouped:\n",
    "        group2 = group2.drop(columns=['cluster'])\n",
    "        similarity = 0\n",
    "        for item_id,user in group.iterrows():\n",
    "            for item_id_2,user2 in group2.iterrows():\n",
    "                if(not user.equals(user2)):\n",
    "                    similarity += np.linalg.norm(user - user2)\n",
    "        print(cluster, cluster2, similarity/(len(group)*(len(group2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to evaluate two main aspects of this group recommender system:\n",
    "- Overall satisfaction of the group\n",
    "    - We will measure this by evaluating individual satisfaction with the items recommended to the group. (nDCG, Recall, Precision, F1-Score - In these metrics, relevant for the user is their top 10 rated movies.)\n",
    "    - Mean of all group members together is the overall satisfaction. \n",
    "- Satisfaction distribution among users. \n",
    "    - Group fairness (DFH)\n",
    "\n",
    "Before initializing, let us define the evaluation.\n",
    "\n",
    "- Decoupled Evaluation. \n",
    "- Non-binarized feedback. \n",
    "- Groundtruth: User satisfaction - We will rank user's top 10 choices and compare them with group's top 5 recommendations. \n",
    "\n",
    "All metrics are calculated with 2000 groups for each group composition strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(recommended_items, relevant_items, k=5):\n",
    "    recommended_set = set(recommended_items[:k])\n",
    "    relevant_set = set(relevant_items)\n",
    "    intersection = recommended_set.intersection(relevant_set)\n",
    "    precision = len(intersection) / k\n",
    "    return precision\n",
    "\n",
    "def get_recall(recommended_items, relevant_items, k=5):\n",
    "    recommended_set = set(recommended_items[:k])\n",
    "    relevant_set = set(relevant_items)\n",
    "    intersection = recommended_set.intersection(relevant_set)\n",
    "    recall = len(intersection) / len(relevant_set) if relevant_set else 0\n",
    "    return recall\n",
    "\n",
    "def ndcg(ranked_list, ground_truth, k=5):\n",
    "    dcg = sum([(1 / np.log2(i + 2)) if item in ground_truth else 0 for i, item in enumerate(ranked_list[:k])])\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(ground_truth)))])\n",
    "    return 0.0 if idcg == 0 else dcg / idcg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoupled evaluation\n",
    "best_centers, grouped = k_means(4, 100) # Get clusters.\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))  \n",
    "axs = axs.ravel()\n",
    "fig2, axs2 = plt.subplots(2, 2, figsize=(15, 10)) \n",
    "axs2 = axs2.ravel()\n",
    "fig3, axs3 = plt.subplots(2, 2, figsize=(15, 10))  \n",
    "axs3 = axs3.ravel()\n",
    "fig4, axs4 = plt.subplots(2, 2, figsize=(15, 10))  \n",
    "axs4 = axs4.ravel()\n",
    "\n",
    "group_generators = [\n",
    "    (divergent_groups, 'Divergent Groups'),\n",
    "    (uniform_groups, 'Uniform Groups'),\n",
    "    (coalitional_groups, 'Coalitional Groups'),\n",
    "    (minority_groups, 'Minority Groups')\n",
    "]\n",
    "\n",
    "group_aggregation_strategies = [\n",
    "    (approval_voting_strategy, \"APP\"),\n",
    "    (additive_utilitarian_strategy, 'ADD'),\n",
    "    (most_pleasure_strategy, 'MPL'),\n",
    "    (least_misery_strategy, 'LMS'),\n",
    "    (majority_strategy,\"MAJ\"),\n",
    "]\n",
    "\n",
    "# for every group composition strategy \n",
    "for idx, (generator, name) in enumerate(group_generators):\n",
    "    print(f\"Processing {name}...\")\n",
    "    groups = generator(grouped, 20)\n",
    "    f1_score_results = []\n",
    "    map_results = []\n",
    "    mrr_results = []\n",
    "    ndcg_results = []\n",
    "    recall_results = []\n",
    "    precision_results = []\n",
    "    aggregation_names = []\n",
    "    n_recommended_movies = 5\n",
    "\n",
    "    # for every aggregation strategy.\n",
    "    for aggregation_strat, name_strat in group_aggregation_strategies:\n",
    "        print(aggregation_strat)\n",
    "        ndcg_list = list()\n",
    "        precision_list = list()\n",
    "        recall_list = list()\n",
    "        f1_score_list = list()\n",
    "        map_score_list = list()\n",
    "        mrr_score_list = list()\n",
    "\n",
    "        for group_id, group in groups.items():\n",
    "            group = group.drop(columns=['cluster'])\n",
    "            sorted_movies_group = aggregation_strat(group, n_recommended_movies)\n",
    "            sorted_movies_group_a = pd.DataFrame({'item': sorted_movies_group.index,})\n",
    "            sorted_movies_group = sorted_movies_group_a['item'].tolist()\n",
    "\n",
    "            for user, ratings in group.iterrows():\n",
    "                cosine_sim_df,sorted_movies_user = getRecommendations(user_plots_ratings_df, user)\n",
    "                sorted_movies_user = sorted_movies_user['item'].tolist()\n",
    "\n",
    "                # Calculate NDCG\n",
    "                ndcg_score = ndcg(sorted_movies_group, sorted_movies_user)\n",
    "                ndcg_list.append(ndcg_score)\n",
    "            \n",
    "                # Calculate Precision\n",
    "                precision = get_precision(sorted_movies_group, sorted_movies_user, n_recommended_movies)\n",
    "                precision_list.append(precision)       \n",
    "\n",
    "                # Calculate Recall         \n",
    "                recall = get_recall(sorted_movies_group, sorted_movies_user, n_recommended_movies)\n",
    "                recall_list.append(recall)\n",
    "\n",
    "                # Calculate F1-Score\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                f1_score_list.append(f1_score)\n",
    "        \n",
    "        avg_ndcg = np.mean(ndcg_list)\n",
    "        avg_precision = np.mean(precision_list)\n",
    "        avg_recall = np.mean(recall_list)\n",
    "        avg_f1_score = np.mean(f1_score_list)\n",
    "        ndcg_results.append(avg_ndcg)\n",
    "        precision_results.append(avg_precision)\n",
    "        recall_results.append(avg_recall)\n",
    "        aggregation_names.append(name_strat)\n",
    "        f1_score_results.append(avg_f1_score)\n",
    "\n",
    "        # Calculate the average of each metric\n",
    "        avg_ndcg = np.mean(ndcg_list)\n",
    "        avg_precision = np.mean(precision_list)\n",
    "        avg_recall = np.mean(recall_list)\n",
    "        avg_f1_score = np.mean(f1_score_list)\n",
    "\n",
    "    axs[idx].bar(aggregation_names, ndcg_results, color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])\n",
    "    axs[idx].set_title(f'NDCG for {name}')\n",
    "    axs[idx].set_ylabel('Average NDCG')\n",
    "    max_ndcg = max(ndcg_results)\n",
    "    offset = max(ndcg_results) * 0.05 \n",
    "    axs[idx].set_ylim([0, max(ndcg_results) + offset * 2])  \n",
    "\n",
    "    for i, value in enumerate(ndcg_results):\n",
    "        axs[idx].text(i, value + offset, round(value, 4), ha='center', va='bottom')\n",
    "\n",
    "    axs2[idx].bar(aggregation_names, recall_results, color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])\n",
    "    axs2[idx].set_title(f'Recall for {name}')\n",
    "    axs2[idx].set_ylabel('Average Recall')\n",
    "    max_ndcg = max(recall_results)\n",
    "    offset = max(recall_results) * 0.05  \n",
    "    axs2[idx].set_ylim([0, max(recall_results) + offset * 2])\n",
    "\n",
    "    for i, value in enumerate(recall_results):\n",
    "        axs2[idx].text(i, value + offset, round(value, 4), ha='center', va='bottom')\n",
    "\n",
    "    axs3[idx].bar(aggregation_names, precision_results, color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])\n",
    "    axs3[idx].set_title(f'Precision for {name}')\n",
    "    axs3[idx].set_ylabel('Average Precision')\n",
    "    max_ndcg = max(precision_results)\n",
    "    offset = max(precision_results) * 0.05 \n",
    "    axs3[idx].set_ylim([0, max(precision_results) + offset * 2]) \n",
    "\n",
    "    for i, value in enumerate(precision_results):\n",
    "        axs3[idx].text(i, value + offset, round(value, 4), ha='center', va='bottom')\n",
    "\n",
    "    axs4[idx].bar(aggregation_names, f1_score_results, color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])\n",
    "    axs4[idx].set_title(f'F1-Score for {name}')\n",
    "    axs4[idx].set_ylabel('Average F1-Score')\n",
    "    max_value = max(f1_score_results)\n",
    "    offset = max_value * 0.05\n",
    "    axs4[idx].set_ylim([0, max_value + offset * 2])\n",
    "\n",
    "    for i, value in enumerate(f1_score_results):\n",
    "        axs4[idx].text(i, value + offset, round(value, 4), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have analysed the overall satisfaction of the group, we must check the satisfaction distribution among users. \n",
    "\n",
    "We will do this by computing group fairness. More specifically, Discounted first hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing fairness\n",
    "import math\n",
    "\n",
    "fig4, axs4 = plt.subplots(2, 2, figsize=(15, 10))  \n",
    "axs4 = axs4.ravel()\n",
    "\n",
    "\n",
    "group_generators = [\n",
    "    (divergent_groups, 'Divergent Groups'),\n",
    "    (uniform_groups, 'Uniform Groups'),\n",
    "    (coalitional_groups, 'Coalitional Groups'),\n",
    "    (minority_groups, 'Minority Groups')\n",
    "]\n",
    "\n",
    "group_aggregation_strategies = [\n",
    "    (approval_voting_strategy, \"APP\"),\n",
    "    (additive_utilitarian_strategy, 'ADD'),\n",
    "    (most_pleasure_strategy, 'MPL'),\n",
    "    (least_misery_strategy, 'LMS'),\n",
    "    (majority_strategy,\"MAJ\"),\n",
    "]\n",
    "n_recommended_movies = 5\n",
    "\n",
    "\n",
    "# for every group composition strategy \n",
    "for idx, (generator, name) in enumerate(group_generators):\n",
    "    print(f\"Processing {name}...\")\n",
    "    groups = generator(grouped, 2000)\n",
    "    dfh_results = []  \n",
    "    aggregation_names = []  \n",
    "\n",
    "    # for every aggregation strategy.\n",
    "    for aggregation_strat, name_strat in group_aggregation_strategies:\n",
    "        dfh_list = list()\n",
    "\n",
    "        for group_id, group in groups.items():\n",
    "            group = group.drop(columns=['cluster'])\n",
    "            sorted_movies_group = aggregation_strat(group,n_recommended_movies)\n",
    "            sorted_movies_group = pd.DataFrame({'item': sorted_movies_group.index,})\n",
    "\n",
    "            dfh_sum = 0 \n",
    "            for user, ratings in group.iterrows():\n",
    "                cosine_sim_df,sorted_movies_user = getRecommendations(user_plots_ratings_df, user)\n",
    "                user_relevant_items = set(sorted_movies_user['item'])  \n",
    "                for rank, item in enumerate(sorted_movies_group['item'], start=1): \n",
    "                    if item in user_relevant_items:\n",
    "                        dfh_sum += 1 / math.log2(rank + 1)\n",
    "                        break  \n",
    "\n",
    "            avg_dfh = dfh_sum / len(group) \n",
    "            dfh_list.append(avg_dfh)  \n",
    "\n",
    "        # Calculate and append overall average DFH for the aggregation strategy\n",
    "        overall_avg_dfh = sum(dfh_list) / len(dfh_list)\n",
    "        dfh_results.append(overall_avg_dfh)\n",
    "        aggregation_names.append(name_strat)        \n",
    "\n",
    "\n",
    "    axs4[idx].bar(aggregation_names, dfh_results, color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])\n",
    "    axs4[idx].set_title(f'DFH for {name}')\n",
    "    axs4[idx].set_ylabel('Average DFH')\n",
    "    max_ndcg = max(dfh_results)\n",
    "    offset = max(dfh_results) * 0.05 \n",
    "    axs4[idx].set_ylim([0, max(dfh_results) + offset * 2]) \n",
    "\n",
    "    for i, value in enumerate(dfh_results):\n",
    "        axs4[idx].text(i, value + offset, round(value, 4), ha='center', va='bottom')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the results from the evaluation \n",
    "\n",
    "EXPERIMENTS ARE RUNNING OVERNIGHT. I WILL WRITE THIS TW. LAST THING LEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding decoupled evaluation bias\n",
    "\n",
    "- Implementing and evaluating several metrics for all group aggregations and compositions was the main action to mitigate DEB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of the explanations is to provide additional information that is associated with the recommendations. This is done to achieve goals such as promoting consensus, increasing transparency, effectiveness, usability and user satisfaction among users. \n",
    "\n",
    "The explanations provided are done based on social choice aggregation. These explanations are focused on reassuring users about the quality and relevance of the recommended movies, and potentially help repair any mistrust or confusion about the system’s functionality or rationale. \n",
    "\n",
    "The explanations for each aggregation strategy are as follows:\n",
    "\n",
    "- “We advise the group to consider movie x.”  -> This is the default for \"no explanation\". \n",
    "\n",
    "\n",
    "ADD \n",
    "- “Movie x has been recommended to the group because it holds the maximum cumulative rating.” - Basic explanation\n",
    "- “Movie x has been recommended to the group because it holds the maximum cumulative rating. This means after adding all group members ratings’ for all movies, movie x had the highest sum.” - Detailed explanation\n",
    "\n",
    "APP \n",
    "- “Movie x has been recommended to the group because it has the highest amount of ratings above threshold θ.”- Basic explanation\n",
    "- “Movie x has been recommended to the group because it has the highest amount of ratings above threshold θ . This means after gathering all group members ratings for each movie, movie x had the most ratings above θ.” - Detailed explanation\n",
    "\n",
    "\n",
    "LMS \n",
    "- “Movie x has been recommended to the group as it doesn't pose significant issues for any member.” - Basic explanation\n",
    "- “Movie x has been recommended to the group as it doesn't pose significant issues for any member. This means that for all movies rated, the highest rating amongst all lowest ratings per movie was the lowest rating of movie x. “- Detailed explanation\n",
    "\n",
    "MPL \n",
    "- “Movie x is recommended to the group as it has received the highest ratings from all members.” - Basic explanation\n",
    "- “Movie x is recommended to the group as it has received the highest ratings from all members. This means that for the highest ratings from all group members for each movie, movie x had the highest rating.“ - Detailed explanation \n",
    "\n",
    "MAJ\n",
    "- \"Movie x is suggested for the group as it is favored by the majority of members.\" - Basic explanation\n",
    "- \"Movie X is suggested for the group as it is favored by the majority of members. This means the majority of the members in the group gave it a high rating.\" - Detailed explanation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_title(movie_id):\n",
    "    movie_row = movies_df[movies_df['item'] == movie_id]\n",
    "    return movie_row.iloc[0]['title']\n",
    "\n",
    "\n",
    "def generate_explanations(group, aggregation_strat, group_id, n_of_recommendations):\n",
    "\n",
    "    if(aggregation_strat == \"ADD\"):\n",
    "        group_results = additive_utilitarian_strategy(group ,n_of_recommendations)\n",
    "        recommended_movies = ', '.join(map(lambda id_: f\"'{get_movie_title(id_)}'\", group_results.index))\n",
    "        if(len(group_results)>1):\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movies {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movies {recommended_movies} have been recommended to group {int(group_id.split('_')[-1])} because they hold the maximum cumulative rating.\")\n",
    "            print(f\"Detailed explanation: Movies {recommended_movies} have been recommended to group {int(group_id.split('_')[-1])} because they hold the maximum cumulative rating. This means after adding all group members ratings’ for all movies, movies {recommended_movies} have the highest sum.\")\n",
    "        else:\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movie {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movie {recommended_movies} has been recommended to group {int(group_id.split('_')[-1])} because it holds the maximum cumulative rating.\")\n",
    "            print(f\"Detailed explanation: Movie {recommended_movies} has been recommended to group {int(group_id.split('_')[-1])} because it holds the maximum cumulative rating. This means after adding all group members ratings’ for all movies, movie {recommended_movies} had the highest sum.\")\n",
    "    if(aggregation_strat == \"APP\"):\n",
    "        group_results = approval_voting_strategy(group, n_of_recommendations)\n",
    "        recommended_movies = ', '.join(map(lambda id_: f\"'{get_movie_title(id_)}'\", group_results.index))\n",
    "        if(len(group_results)>1):\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movies {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movies {recommended_movies} have been recommended to group {int(group_id.split('_')[-1])} because they have the highest amount of ratings above threshold θ.\")\n",
    "            print(f\"Detailed explanation: Movies {recommended_movies} have been recommended to group {int(group_id.split('_')[-1])} because they have the highest amount of ratings above threshold θ . This means after gathering all group members ratings for each movie, movies {recommended_movies} have the most ratings above θ.\")\n",
    "        else:\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movie {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movie {recommended_movies} has been recommended to group {int(group_id.split('_')[-1])} because it has the highest amount of ratings above threshold θ.\")\n",
    "            print(f\"Detailed explanation: Movie {recommended_movies} has been recommended to group {int(group_id.split('_')[-1])} because it has the highest amount of ratings above threshold θ . This means after gathering all group members ratings for each movie, movie {recommended_movies} had the most ratings above θ.\")\n",
    "    if(aggregation_strat == \"LMS\"):\n",
    "        group_results = least_misery_strategy(group, n_of_recommendations)\n",
    "        recommended_movies = ', '.join(map(lambda id_: f\"'{get_movie_title(id_)}'\", group_results.index))\n",
    "        if(len(group_results)>1):\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movies {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movies {recommended_movies} have been recommended to group {int(group_id.split('_')[-1])}) as they don't pose significant issues for any member.\")\n",
    "            print(f\"Detailed explanation: Movies {recommended_movies} have been recommended to group {int(group_id.split('_')[-1])}) as they don't pose significant issues for any member. This means that for all movies rated, the highest ratings amongst all lowest ratings per movie was the lowest rating of movies {recommended_movies}.\")\n",
    "        else:\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movie {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movie {recommended_movies} has been recommended to group {int(group_id.split('_')[-1])} as it doesn't pose significant issues for any member.\")\n",
    "            print(f\"Detailed explanation: Movie {recommended_movies} has been recommended to group {int(group_id.split('_')[-1])} as it doesn't pose significant issues for any member. This means that for all movies rated, the highest rating amongst all lowest ratings per movie was the lowest rating of movie {recommended_movies}.\")\n",
    "    if(aggregation_strat == \"MPL\"):\n",
    "        group_results = most_pleasure_strategy(group, n_of_recommendations)\n",
    "        recommended_movies = ', '.join(map(lambda id_: f\"'{get_movie_title(id_)}'\", group_results.index))\n",
    "        if(len(group_results)>1):\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movies {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movies {recommended_movies} are recommended to group {int(group_id.split('_')[-1])} as they have received the highest ratings from all members.\")\n",
    "            print(f\"Detailed explanation: Movies {recommended_movies} are recommended to group {int(group_id.split('_')[-1])} as they have received the highest ratings from all members. This means that for the highest ratings from all group members for each movie, movies {recommended_movies} had the highest rating.\")\n",
    "        else:\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movie {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movie {recommended_movies} is recommended to group {int(group_id.split('_')[-1])} as it has received the highest ratings from all members.\")\n",
    "            print(f\"Detailed explanation: Movie {recommended_movies} is recommended to group {int(group_id.split('_')[-1])} as it has received the highest ratings from all members. This means that for the highest ratings from all group members for each movie, movie {recommended_movies} had the highest rating.\")\n",
    "    if(aggregation_strat == \"MAJ\"):\n",
    "        group_results = majority_strategy(group, n_of_recommendations)\n",
    "        recommended_movies = ', '.join(map(lambda id_: f\"'{get_movie_title(id_)}'\", group_results.index))\n",
    "        if(len(group_results)>1):\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movies {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movies {recommended_movies} are suggested for group {int(group_id.split('_')[-1])} as they are favored by the majority of members.\")\n",
    "            print(f\"Detailed explanation: Movies {recommended_movies} are suggested for group {int(group_id.split('_')[-1])} as  they are favored by the majority of members. This means the majority of the members in the group gave them a high rating.\")\n",
    "        else:\n",
    "            print(f\"No explanation: We advise group {int(group_id.split('_')[-1])} to consider movie {recommended_movies}.\")\n",
    "            print(f\"Basic explanation: Movie {recommended_movies} is suggested for group {int(group_id.split('_')[-1])} as it is favored by the majority of members.\")\n",
    "            print(f\"Detailed explanation: Movie {recommended_movies} is suggested for group {int(group_id.split('_')[-1])} as it is favored by the majority of members. This means the majority of the members in the group gave it a high rating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of an explanation generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No explanation: We advise group 1 to consider movies 'taxi driver', 'rear window'.\n",
      "Basic explanation: Movies 'taxi driver', 'rear window' are suggested for group 1 as they are favored by the majority of members.\n",
      "Detailed explanation: Movies 'taxi driver', 'rear window' are suggested for group 1 as  they are favored by the majority of members. This means the majority of the members in the group gave them a high rating.\n",
      "_________________________\n",
      "No explanation: We advise group 2 to consider movies 'paths of glory', 'toy story'.\n",
      "Basic explanation: Movies 'paths of glory', 'toy story' are suggested for group 2 as they are favored by the majority of members.\n",
      "Detailed explanation: Movies 'paths of glory', 'toy story' are suggested for group 2 as  they are favored by the majority of members. This means the majority of the members in the group gave them a high rating.\n",
      "_________________________\n",
      "No explanation: We advise group 3 to consider movies 'toy story', 'taxi driver'.\n",
      "Basic explanation: Movies 'toy story', 'taxi driver' are suggested for group 3 as they are favored by the majority of members.\n",
      "Detailed explanation: Movies 'toy story', 'taxi driver' are suggested for group 3 as  they are favored by the majority of members. This means the majority of the members in the group gave them a high rating.\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "groups = minority_groups(grouped, 3) \n",
    "\n",
    "for group_id, group in groups.items(): \n",
    "    generate_explanations(group, \"MAJ\", group_id, 2)\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Explanations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
